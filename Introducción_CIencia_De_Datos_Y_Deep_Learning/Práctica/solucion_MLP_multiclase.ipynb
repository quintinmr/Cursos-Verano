{"cells":[{"cell_type":"markdown","metadata":{"id":"AP_S7sk_u6dN"},"source":["## Ejercicio: Modificar el programa completo para en entrenamiento, validación y test con 7 clases \n","## Solución"]},{"cell_type":"markdown","metadata":{"id":"gxKBD2Bxig9J"},"source":["**Carga y descompresión de datos**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10747,"status":"ok","timestamp":1660664839270,"user":{"displayName":"Andres Ortiz","userId":"09780072354748945801"},"user_tz":-120},"id":"Y6c38Q_mieET"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-08-16 16:33:40--  https://docs.google.com/uc?export=download\u0026confirm=\u0026id=1-LFuqLydW7zwEFbaci54SJKTWzNIr9OU\n","Resolving docs.google.com (docs.google.com)... 142.251.10.101, 142.251.10.113, 142.251.10.139, ...\n","Connecting to docs.google.com (docs.google.com)|142.251.10.101|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘olivenet.zip’\n","\n","olivenet.zip            [ \u003c=\u003e                ]   1.95K  --.-KB/s    in 0s      \n","\n","2022-08-16 16:33:40 (22.6 MB/s) - ‘olivenet.zip’ saved [1993]\n","\n","mkdir: cannot create directory ‘data’: File exists\n","Archive:  olivenet.zip\n","  End-of-central-directory signature not found.  Either this file is not\n","  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n","  latter case the central directory and zipfile comment will be found on\n","  the last disk(s) of this archive.\n","unzip:  cannot find zipfile directory in one of olivenet.zip or\n","        olivenet.zip.zip, and cannot find olivenet.zip.ZIP, period.\n"]}],"source":["#@title Desarga y descompresión de datos\n","data_id = \"variedades\" #@param [\"variedades\", \"olivenet\"]\n","\n","correspond = {'variedades': '1nM-EhT6ejaYtDbAvm0kbNScd6hyYzKsS', \n","              'olivenet': '1-LFuqLydW7zwEFbaci54SJKTWzNIr9OU'}\n","if data_id=='variedades':\n","  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=1nM-EhT6ejaYtDbAvm0kbNScd6hyYzKsS' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=1nM-EhT6ejaYtDbAvm0kbNScd6hyYzKsS\" -O 'variedades.zip' \u0026\u0026 rm -rf /tmp/cookies.txt\n","elif data_id=='olivenet':\n","  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download\u0026id=1-LFuqLydW7zwEFbaci54SJKTWzNIr9OU' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\u0026id=1-LFuqLydW7zwEFbaci54SJKTWzNIr9OU\" -O 'olivenet.zip' \u0026\u0026 rm -rf /tmp/cookies.txt\n","else:\n","  print('wrong id')\n","!mkdir data\n","!unzip {data_id}.zip -d data"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10283,"status":"ok","timestamp":1660664895210,"user":{"displayName":"Andres Ortiz","userId":"09780072354748945801"},"user_tz":-120},"id":"U488Pp4vu6dQ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from torchvision.utils import make_grid\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import matplotlib.pyplot as plt\n","datadir=\"/content/data/Variedades-JPG/ENTRENAMIENTO_45_GREYSCALE\"\n","\n","# Definimos el modelo\n","\n","class MLP3(nn.Module):\n","    def __init__(self,input_neurons=41*41):\n","        super(MLP3, self).__init__()\n","        self.input_neurons=input_neurons\n","        self.fc1 = nn.Linear(self.input_neurons, 512)      # Pesos Entrada - Oculta 1\n","        self.fc2 = nn.Linear(512, 1024 )                   # Pesos Oculta 1 - Oculta 2\n","        self.fc3 = nn.Linear(1024, 7)                      # Pesos Oculta 2 - Salida. 7 Clases -\u003e 7 neuronas de salida !!!\n","        \n","    def forward(self, x):\n","        x = x.view(-1, self.input_neurons)                 #transforma las imágenes de tamaño (41x41x3) a (n, 41x41x3)\n","        x = F.relu(self.fc1(x))                            #Función de activación relu en la salida de la capa oculta 1\n","        x = F.relu(self.fc2(x))                            #Función de activación relu en la salida de la capa oculta 2\n","        x = F.softmax(self.fc3(x), dim=1)                  #Función de activación softmax en la salida\n","        return x\n","\n","# Carga dataset y transforma objetos PIL que devuelve Image Folder a Tensores (PyTorch)\n","dataset = ImageFolder(root=datadir,transform=transforms.ToTensor())\n","# Genera modelo\n","model=MLP3(3*501*501).to('cuda')   # Las imágenes ahora son de 501x501 pixels\n","# Define función de loss\n","criterion = nn.CrossEntropyLoss() # definimos la pérdida\n","\n","# Utilizamos descenso de gradiente estocástico con un learning-rate \n","# (factor que cuantifica cuánto vamos a actualizar los pesos con respecto\n","# a su valor actual) de 0.01 y un momento de 0.9, que actualiza el learning\n","# rate en función de sus valores anteriores\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# Definimos el objeto que genera los índices para hacer validación cruzada \n","#skf1 = KFold(shuffle=True)\n","#skf2 = StratifiedKFold(n_splits=5)\n","skf3 = StratifiedShuffleSplit(n_splits=1, train_size=0.1, test_size=0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJMd79V3u6dS"},"outputs":[],"source":["La codificación por defecto que le asigna ImageFolder:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1660664918832,"user":{"displayName":"Andres Ortiz","userId":"09780072354748945801"},"user_tz":-120},"id":"cGanNKR4u6dT","outputId":"10bc3146-4962-46b9-a545-f5bed2aab7aa"},"outputs":[{"data":{"text/plain":["{'Arbequina': 0,\n"," 'Arbosana': 1,\n"," 'Changlot': 2,\n"," 'Lechin': 3,\n"," 'Ocal': 4,\n"," 'Picual': 5,\n"," 'Verdial': 6}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset.class_to_idx"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":543,"status":"ok","timestamp":1660664923787,"user":{"displayName":"Andres Ortiz","userId":"09780072354748945801"},"user_tz":-120},"id":"Flddpa6Zu6dU"},"outputs":[],"source":["######################################################################################################################\n","# Funciones auxiliares. \n","#       Calculo de estadísticas\n","#       Curva de aprendizaje\n","######################################################################################################################\n","from sklearn.metrics import confusion_matrix, roc_curve, auc\n","import numpy as np\n","\n","def clasif_perf(predicted,actual):\n","    # Cálculo de estadísticas y métricas de clasificación\n","    cm = confusion_matrix(actual,predicted)\n","    # from confusion matrix calculate acc, sens, spec\n","    total=sum(sum(cm))\n","    acc=(cm[0,0]+cm[1,1])/total\n","    sens = cm[0,0]/(cm[0,0]+cm[0,1])\n","    spec = cm[1,1]/(cm[1,0]+cm[1,1])\n","    return acc,sens,spec \n","\n","def plot_roc(true_labels,score,posclas,roc_color='r',lab_legend='ROC CURVE',last=True):\n","    fpr, tpr, _ = roc_curve(true_labels, score,pos_label=posclas)\n","    roc_auc = auc(fpr.astype(float), tpr.astype(float))\n","    #plt.figure()\n","    lw=2\n","    plt.plot(fpr, tpr, color=roc_color,\n","             lw=lw, label=lab_legend+' (AUC = %0.2f)' % roc_auc)\n","    if last:\n","        plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--',label='Random Classifier')\n","\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.0])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","    print('AUC=%1.3f'%roc_auc)\n","    return roc_auc\n","\n","\n","def plot_learning_curve(train_history,validation_history):\n","    # Plot Learning Curve\n","    fig=plt.figure()\n","    plt.plot(np.arange(len(train_history)),train_history,'b-',label='Training')\n","    plt.plot(np.arange(len(val_history)),val_history,'r-',label='Validation')\n","    plt.legend(loc='upper right',fontsize=14)\n","    plt.xlabel(\"epoch\",fontsize=14)\n","    plt.ylabel(\"Loss\",fontsize=14)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":956},"executionInfo":{"elapsed":540155,"status":"error","timestamp":1660665468397,"user":{"displayName":"Andres Ortiz","userId":"09780072354748945801"},"user_tz":-120},"id":"Kraz9aR0u6dW","outputId":"5a2fefc7-e6d0-4e86-f680-0746091a403f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003e\u003e\u003e Epoch 0 \u003e\u003e\u003e\u003e Train Loss: 0.004200769141155262, Train Acc: 0.14419610798358917, Val Loss: 0.005551018185849148, Val Acc: 0.1974063366651535\n","\u003e\u003e\u003e Epoch 1 \u003e\u003e\u003e\u003e Train Loss: 0.004139745759723336, Train Acc: 0.20980532467365265, Val Loss: 0.00540724405294193, Val Acc: 0.2708933651447296\n","\u003e\u003e\u003e Epoch 2 \u003e\u003e\u003e\u003e Train Loss: 0.0040431101715813975, Train Acc: 0.27901947498321533, Val Loss: 0.005293806107655726, Val Acc: 0.30691641569137573\n","\u003e\u003e\u003e Epoch 3 \u003e\u003e\u003e\u003e Train Loss: 0.003968636166215717, Train Acc: 0.3229992687702179, Val Loss: 0.005205872423023587, Val Acc: 0.34149855375289917\n","\u003e\u003e\u003e Epoch 4 \u003e\u003e\u003e\u003e Train Loss: 0.0039038302319120313, Train Acc: 0.3597692847251892, Val Loss: 0.005124949584433256, Val Acc: 0.37319883704185486\n","\u003e\u003e\u003e Epoch 5 \u003e\u003e\u003e\u003e Train Loss: 0.00384745283271223, Train Acc: 0.3842826187610626, Val Loss: 0.005058163868247261, Val Acc: 0.42363110184669495\n","\u003e\u003e\u003e Epoch 6 \u003e\u003e\u003e\u003e Train Loss: 0.0038040138098861817, Train Acc: 0.43330928683280945, Val Loss: 0.00502308129577197, Val Acc: 0.4193083345890045\n","\u003e\u003e\u003e Epoch 7 \u003e\u003e\u003e\u003e Train Loss: 0.0037759459096060096, Train Acc: 0.42393654584884644, Val Loss: 0.004997582875342466, Val Acc: 0.4207492768764496\n","\u003e\u003e\u003e Epoch 8 \u003e\u003e\u003e\u003e Train Loss: 0.003760538087444677, Train Acc: 0.43042537569999695, Val Loss: 0.004977841026844827, Val Acc: 0.4322766363620758\n","\u003e\u003e\u003e Epoch 9 \u003e\u003e\u003e\u003e Train Loss: 0.003739619048791733, Train Acc: 0.4318673312664032, Val Loss: 0.004953696164342786, Val Acc: 0.4337175786495209\n","\u003e\u003e\u003e Epoch 10 \u003e\u003e\u003e\u003e Train Loss: 0.003727656349314144, Train Acc: 0.45133379101753235, Val Loss: 0.004943209525831151, Val Acc: 0.4495677053928375\n","\u003e\u003e\u003e Epoch 11 \u003e\u003e\u003e\u003e Train Loss: 0.003706557680224891, Train Acc: 0.4628694951534271, Val Loss: 0.004913962540090599, Val Acc: 0.4639769196510315\n","\u003e\u003e\u003e Epoch 12 \u003e\u003e\u003e\u003e Train Loss: 0.0036905160806294216, Train Acc: 0.4657534062862396, Val Loss: 0.004907912582760242, Val Acc: 0.45389047265052795\n","\u003e\u003e\u003e Epoch 13 \u003e\u003e\u003e\u003e Train Loss: 0.003680330777047947, Train Acc: 0.4657534062862396, Val Loss: 0.004879798600584354, Val Acc: 0.46109509468078613\n","\u003e\u003e\u003e Epoch 14 \u003e\u003e\u003e\u003e Train Loss: 0.0036651467623775953, Train Acc: 0.471521258354187, Val Loss: 0.004879137624581197, Val Acc: 0.46109509468078613\n","\u003e\u003e\u003e Epoch 15 \u003e\u003e\u003e\u003e Train Loss: 0.003654074307904453, Train Acc: 0.4744051694869995, Val Loss: 0.0048501728591039475, Val Acc: 0.4755043089389801\n","\u003e\u003e\u003e Epoch 16 \u003e\u003e\u003e\u003e Train Loss: 0.0036397650676746546, Train Acc: 0.4794520437717438, Val Loss: 0.004854119820278728, Val Acc: 0.47262245416641235\n","\u003e\u003e\u003e Epoch 17 \u003e\u003e\u003e\u003e Train Loss: 0.0036307757246296615, Train Acc: 0.4960345923900604, Val Loss: 0.004820904917263504, Val Acc: 0.48559075593948364\n","\u003e\u003e\u003e Epoch 18 \u003e\u003e\u003e\u003e Train Loss: 0.003611368331964186, Train Acc: 0.4974765479564667, Val Loss: 0.004839552582512671, Val Acc: 0.4827089309692383\n","\u003e\u003e\u003e Epoch 19 \u003e\u003e\u003e\u003e Train Loss: 0.003614575895544601, Train Acc: 0.5003604888916016, Val Loss: 0.00481381694590324, Val Acc: 0.49567723274230957\n","\u003e\u003e\u003e Epoch 20 \u003e\u003e\u003e\u003e Train Loss: 0.0036088093288011835, Train Acc: 0.4974765479564667, Val Loss: 0.004841393283876974, Val Acc: 0.4812679886817932\n","\u003e\u003e\u003e Epoch 21 \u003e\u003e\u003e\u003e Train Loss: 0.0036186734462867304, Train Acc: 0.49026674032211304, Val Loss: 0.0047978471953861995, Val Acc: 0.5057636499404907\n","\u003e\u003e\u003e Epoch 22 \u003e\u003e\u003e\u003e Train Loss: 0.0035908997445632507, Train Acc: 0.5068492889404297, Val Loss: 0.004761815242877268, Val Acc: 0.5028818249702454\n","\u003e\u003e\u003e Epoch 23 \u003e\u003e\u003e\u003e Train Loss: 0.0035570385478431573, Train Acc: 0.5169430375099182, Val Loss: 0.004757700289360728, Val Acc: 0.5043227672576904\n","\u003e\u003e\u003e Epoch 24 \u003e\u003e\u003e\u003e Train Loss: 0.0035506991404978252, Train Acc: 0.5255947709083557, Val Loss: 0.0047544397263430726, Val Acc: 0.5129683017730713\n","\u003e\u003e\u003e Epoch 25 \u003e\u003e\u003e\u003e Train Loss: 0.003543364202365999, Train Acc: 0.5306416749954224, Val Loss: 0.004747149580150241, Val Acc: 0.5100864171981812\n","\u003e\u003e\u003e Epoch 26 \u003e\u003e\u003e\u003e Train Loss: 0.0035391862046916275, Train Acc: 0.5241528153419495, Val Loss: 0.004732483058566662, Val Acc: 0.5244956612586975\n","\u003e\u003e\u003e Epoch 27 \u003e\u003e\u003e\u003e Train Loss: 0.0035303418552093398, Train Acc: 0.5328046083450317, Val Loss: 0.0047428635424083525, Val Acc: 0.5115273594856262\n","\u003e\u003e\u003e Epoch 28 \u003e\u003e\u003e\u003e Train Loss: 0.0035274504068177987, Train Acc: 0.5328046083450317, Val Loss: 0.004730353781400565, Val Acc: 0.5100864171981812\n","\u003e\u003e\u003e Epoch 29 \u003e\u003e\u003e\u003e Train Loss: 0.0035214883952838704, Train Acc: 0.5356885194778442, Val Loss: 0.00473201206163302, Val Acc: 0.5259366035461426\n","\u003e\u003e\u003e Epoch 30 \u003e\u003e\u003e\u003e Train Loss: 0.0035262012447310424, Train Acc: 0.5335255861282349, Val Loss: 0.004716742622749263, Val Acc: 0.5158501267433167\n","\u003e\u003e\u003e Epoch 31 \u003e\u003e\u003e\u003e Train Loss: 0.0035060749005824433, Train Acc: 0.5414563417434692, Val Loss: 0.004734403118276459, Val Acc: 0.5100864171981812\n","\u003e\u003e\u003e Epoch 32 \u003e\u003e\u003e\u003e Train Loss: 0.0035194258253266678, Train Acc: 0.5371304750442505, Val Loss: 0.004713017074793835, Val Acc: 0.5172910690307617\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-5-321552b79ad7\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Ahora vamos con el bucle de validación para evaluar el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 44\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mtotal_val\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 232\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Bucle sobre los subconjuntos generados. Este código en general, podría utilizarse para k\u003e1\n","n_epochs = 100   # 100 iteraciones en el entrenamiento\n","for i_fold, (train_idx, val_idx) in enumerate(skf3.split(dataset,dataset.targets)):\n","    # Datasets de train y test\n","    dataset_train = torch.utils.data.Subset(dataset, train_idx)\n","    dataset_validation = torch.utils.data.Subset(dataset, val_idx)\n","    \n","    # Creamos los dataloaders de train y test\n","    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=500, shuffle=False, pin_memory=True)\n","    validation_loader = torch.utils.data.DataLoader(dataset_validation, batch_size=500, shuffle=False, pin_memory=True)\n","    train_history=[]\n","    val_history=[]\n","    # Iteramos para actualizar los pesos: entrenamiento\n","    for epoch in range(n_epochs):\n","        train_loss = 0.0 # el loss en cada epoch de entrenamiento\n","        train_acc = 0.0 # el accuracy de cada epoch\n","        val_loss = 0.0 # el loss en cada epoch de validación\n","        val_acc = 0.0 # el accuracy de cada epoch de validación\n","        total_train=0\n","        total_val=0\n","\n","    \n","        # Iteramos con el trainloader\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            total_train += labels.shape[0]\n","            labels=labels.to('cuda')\n","            optimizer.zero_grad()              # ponemos a cero todos los gradientes en todas las neuronas\n","    \n","            images=images.view(-1,3*501*501)\n","            outputs = model(images.to('cuda'))            # Propagación hacia delante (Feed Forward) \n","            loss = criterion(outputs, labels)  # Estimamos el loss (error)\n","            loss.backward()                               # Propagación hacia atrás (backprop)\n","            optimizer.step()                              # Optimización de los pesos\n","            \n","            # Mostramos las estadísticas conforme entrenamos la red\n","            train_loss += loss.item() # acumulamos el loss de este batch\n","            # extraemos las etiquetas que predice (nº neurona con máxima probabilidad)\n","            _, predicted = torch.max(outputs, 1) \n","            train_acc += torch.sum(predicted==labels) # y acumulamos el número de correctos\n","        \n","        train_history.append(train_loss/total_train)\n","        \n","        # Ahora vamos con el bucle de validación para evaluar el modelo \n","        for j, (images,labels) in enumerate(validation_loader,0):\n","            total_val+= labels.shape[0]\n","            labels=labels.to('cuda')\n","            images=images.view(-1,3*501*501)\n","            outputs = model(images.to('cuda'))     \n","            loss = criterion(outputs, labels)  # Estimamos el loss (error)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1) \n","            val_acc += torch.sum(predicted==labels)\n","            \n","        val_history.append(val_loss/total_val)\n","        print(f'\u003e\u003e\u003e Epoch {epoch} \u003e\u003e\u003e\u003e Train Loss: {train_loss/total_train}, Train Acc: {train_acc/total_train}, Val Loss: {val_loss/total_val}, Val Acc: {val_acc/total_val}')"]},{"cell_type":"markdown","metadata":{"id":"F8EW_VZHu6dY"},"source":["Mostramos la curva de aprendizaje"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1660665544365,"user":{"displayName":"Andres Ortiz","userId":"09780072354748945801"},"user_tz":-120},"id":"hQ2AKwhlu6dZ","outputId":"66e3334a-322c-44f7-8a80-c76f55166217"},"outputs":[{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 432x288 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["plot_learning_curve(train_history,val_history)"]},{"cell_type":"markdown","metadata":{"id":"3e3sHR2eu6dZ"},"source":["Vemos que obtenemos un accuracy bastante bajo (cercano al 50%) y que el modelo no es capaz de generalizar (loss de validación muy alto). Esto es porque la técnica **Pixel as Features** utiliza como características las propias intensidades. No utiliza información de vecindad ni estadísticos de orden superior. En la siguiente sesión del curso, veremos que esto tiene dos soluciones:\n","1. Extraer características predeterminadas de las imágenes (intensidad media, varianza....) en diferentes partes de cada images y utilizamos esas características a la entrada del MLP\n","2. Utilizar una red neuronal que sea capaz de extraer características ad-hoc, que resulten suficientemente descriptivas para el problema que estamos tratando :)\n"]},{"cell_type":"markdown","metadata":{"id":"9ndTzZe5u6da"},"source":["Una vez entrenado el modelo, estimamos su capacidad de generalización con el dataset de test.\n","Para poder calcular la curva ROC necesitamos un score y la etiqueta \"ground truth\". El score lo obtenemos restando los dos valores que proporciona softmax (la activación para cada neurona de salida).\n","Almacenamos estos scores en la variable *scores*, y guardamos las etiquetas \"ground truth\" en *lab_mat* así como las predicciones de la red en *pred_mat*"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8239,"status":"ok","timestamp":1660665558818,"user":{"displayName":"Andres Ortiz","userId":"09780072354748945801"},"user_tz":-120},"id":"k-2SwgVVu6da","outputId":"82157119-f4d4-4e79-c6f0-fcc50ac45e63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precisión del modelo en las imágenes de test: 0.5971428571428572\n"]}],"source":["dataset_test = ImageFolder(root='/content/data/Variedades-JPG/VALIDACION_45_GREYSCALE',transform=transforms.ToTensor())\n","# Creamos el dataloader\n","test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=500, shuffle=False)\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():       # hay que deshabilitar la propagación de gradiente\n","    for i, (images, labels) in enumerate(test_loader,0):\n","        outputs = model(images.to('cuda'))                 # Propagación hacia delante\n","        _, predicted = torch.max(outputs, 1)    # obtención de etiquetas numéricas\n","        total += labels.size(0)                 # aumentamos el número de etiquetas comprobadas para calcular la precisión después\n","        correct += torch.sum(predicted == labels.to('cuda')).item() # sumamos el número de etiquetas correctas para calcular la precisión\n","        if i==0:\n","            pred_mat=predicted.detach().cpu().numpy()\n","            lab_mat=labels.detach().cpu().numpy()\n","            scores=np.diff(outputs.detach().cpu().numpy()).flatten()\n","        else:\n","            pred_mat=np.hstack((pred_mat,predicted.detach().cpu().numpy()))\n","            lab_mat=np.hstack((lab_mat,labels.detach().cpu().numpy()))\n","            scores=np.hstack((scores,np.diff(outputs.detach().cpu().numpy()).flatten()))\n","\n","\n","print(f'Precisión del modelo en las imágenes de test: {correct / total}')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"solucion_MLP_multiclase.ipynb","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
